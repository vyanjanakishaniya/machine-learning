{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99ac7be952eaadb29751bc9a6ed29f20",
     "grade": false,
     "grade_id": "cell-cb5118d731164dd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Programming Assignment 2\n",
    "* CSCI-5930 ML Fall 2022\n",
    "* Author: Dr. B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "924972ad87bdd98a61e260f99cf8685c",
     "grade": false,
     "grade_id": "cell-e5ed87be36e78162",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Rules to work with this notebook\n",
    "* You can create new cells in this notebook.\n",
    "* PLEASE DO NOT DELETE ANY EXISTING CELL FROM THIS NOTEBOOK.\n",
    "* In a cell, you are allowed only to write codes/snippets right after the marking like ##YOUR CODE. You are not allowed to write anywhere else in that particular cell.\n",
    "* Your work for each task has to be in the specific cells with the ##YOUR CODE HERE marks, otherwise your submission won't be graded. Please do not delete the marks.\n",
    "* After the ##YOUR CODE HERE marks, you may see command `raise NotImplementedError()` exception that alerts us that you did not attempt a particular task. Be sure to delete that command if you attempt to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85d56ec8be83f2eefb7bb55edd6e6b2d",
     "grade": false,
     "grade_id": "cell-68cbcbe2b3d9eff7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: appnope==0.1.3 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.1.3)\r\n",
      "Requirement already satisfied: asttokens==2.0.8 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.0.8)\r\n",
      "Requirement already satisfied: backcall==0.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.2.0)\r\n",
      "Requirement already satisfied: debugpy==1.6.3 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.6.3)\r\n",
      "Requirement already satisfied: decorator==5.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (5.1.1)\r\n",
      "Requirement already satisfied: entrypoints==0.4 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.4)\r\n",
      "Requirement already satisfied: executing==1.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.0.0)\r\n",
      "Requirement already satisfied: ipykernel==6.15.3 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (6.15.3)\r\n",
      "Requirement already satisfied: ipython==8.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (8.5.0)\r\n",
      "Requirement already satisfied: jedi==0.18.1 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (0.18.1)\r\n",
      "Requirement already satisfied: joblib==1.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (1.2.0)\r\n",
      "Requirement already satisfied: jupyter-core==4.11.1 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (4.11.1)\r\n",
      "Requirement already satisfied: jupyter_client==7.3.5 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (7.3.5)\r\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (0.1.6)\r\n",
      "Requirement already satisfied: nest-asyncio==1.5.5 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (1.5.5)\r\n",
      "Requirement already satisfied: numpy==1.23.3 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (1.23.3)\r\n",
      "Requirement already satisfied: packaging==21.3 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (21.3)\r\n",
      "Requirement already satisfied: pandas==1.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (1.5.0)\r\n",
      "Requirement already satisfied: parso==0.8.3 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (0.8.3)\r\n",
      "Requirement already satisfied: pexpect==4.8.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (4.8.0)\r\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit==3.0.31 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (3.0.31)\r\n",
      "Requirement already satisfied: psutil==5.9.2 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (5.9.2)\r\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (0.7.0)\r\n",
      "Requirement already satisfied: pure-eval==0.2.2 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (0.2.2)\r\n",
      "Requirement already satisfied: Pygments==2.13.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (2.13.0)\r\n",
      "Requirement already satisfied: pyparsing==3.0.9 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (2.8.2)\r\n",
      "Requirement already satisfied: pytz==2022.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (2022.2.1)\r\n",
      "Requirement already satisfied: pyzmq==24.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (24.0.0)\r\n",
      "Requirement already satisfied: scikit-learn==1.1.2 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (1.1.2)\r\n",
      "Requirement already satisfied: scipy==1.9.1 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (1.9.1)\r\n",
      "Requirement already satisfied: six==1.16.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (1.16.0)\r\n",
      "Requirement already satisfied: stack-data==0.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (0.5.0)\r\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (3.1.0)\r\n",
      "Requirement already satisfied: tornado==6.2 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (6.2)\r\n",
      "Requirement already satisfied: traitlets==5.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (5.4.0)\r\n",
      "Requirement already satisfied: wcwidth==0.2.5 in /opt/anaconda3/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (0.2.5)\r\n"
     ]
    }
   ],
   "source": [
    "#Please make sure you are using a python3.9.x interpreter \n",
    "# and install the packages from requirements.txt (provided)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3484954312538f15962241111877aac8",
     "grade": false,
     "grade_id": "cell-bd2e3291ee874ec0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Some basic imports. You can import any package at any cell\n",
    "from sklearn.preprocessing import OneHotEncoder   #My favorite categorical to numerical feature conversion tool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import inv\n",
    "from time import perf_counter\n",
    "import math\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3017f74861738fb895c8a48f31652023",
     "grade": false,
     "grade_id": "cell-f3022bc3123b69be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Let's load the given dataset\n",
    "# First load the dataset into pandas dataframe\n",
    "full_dataset = pd.read_csv('dataset/baby-weights-dataset.csv',delimiter=',')\n",
    "judge_dataset = pd.read_csv('dataset/judge-without-labels.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SEX</th>\n",
       "      <th>MARITAL</th>\n",
       "      <th>FAGE</th>\n",
       "      <th>GAINED</th>\n",
       "      <th>VISITS</th>\n",
       "      <th>MAGE</th>\n",
       "      <th>FEDUC</th>\n",
       "      <th>MEDUC</th>\n",
       "      <th>TOTALP</th>\n",
       "      <th>...</th>\n",
       "      <th>HEMOGLOB</th>\n",
       "      <th>HYPERCH</th>\n",
       "      <th>HYPERPR</th>\n",
       "      <th>ECLAMP</th>\n",
       "      <th>CERVIX</th>\n",
       "      <th>PINFANT</th>\n",
       "      <th>PRETERM</th>\n",
       "      <th>RENAL</th>\n",
       "      <th>RHSEN</th>\n",
       "      <th>UTERINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1996</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1998</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  SEX  MARITAL  FAGE  GAINED  VISITS  MAGE  FEDUC  MEDUC  TOTALP  \\\n",
       "0        1    1        1    30      70      11    26     14     16       4   \n",
       "1        2    2        2    21      36      15    18      9     12       1   \n",
       "2        3    2        1    22      18      10    25     12     12       5   \n",
       "3        4    2        1    24      25      10    22     12     11       4   \n",
       "4        5    2        1    24      38      15    26     12     12       1   \n",
       "...    ...  ...      ...   ...     ...     ...   ...    ...    ...     ...   \n",
       "1995  1996    2        1    21      17       9    21     13     12       1   \n",
       "1996  1997    1        1    27      20      10    26     16     16       2   \n",
       "1997  1998    1        1    29      25      17    27     12     12       2   \n",
       "1998  1999    1        1    37      29      11    34     12     16       2   \n",
       "1999  2000    1        2    30      20      13    27     12     13       2   \n",
       "\n",
       "      ...  HEMOGLOB  HYPERCH  HYPERPR  ECLAMP  CERVIX  PINFANT PRETERM RENAL  \\\n",
       "0     ...         0        0        0       0       0        0       0     0   \n",
       "1     ...         1        0        0       0       0        0       0     0   \n",
       "2     ...         0        0        0       0       0        0       0     0   \n",
       "3     ...         0        0        0       0       0        0       0     0   \n",
       "4     ...         0        0        1       0       0        0       0     0   \n",
       "...   ...       ...      ...      ...     ...     ...      ...     ...   ...   \n",
       "1995  ...         0        0        0       0       0        0       0     0   \n",
       "1996  ...         0        0        0       0       0        0       0     1   \n",
       "1997  ...         0        0        0       0       0        0       0     0   \n",
       "1998  ...         0        0        0       0       0        0       0     0   \n",
       "1999  ...         0        0        0       0       0        0       0     0   \n",
       "\n",
       "      RHSEN  UTERINE  \n",
       "0         0        0  \n",
       "1         0        0  \n",
       "2         0        0  \n",
       "3         0        0  \n",
       "4         0        0  \n",
       "...     ...      ...  \n",
       "1995      0        0  \n",
       "1996      0        0  \n",
       "1997      0        0  \n",
       "1998      0        0  \n",
       "1999      0        0  \n",
       "\n",
       "[2000 rows x 36 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: \n",
    "Separate the full_dataset into two parts: X and y, where X denotes the input matrix containing only the input (i.e., independent explanatory) variables, and y denotes the target variable containing only the target values for exactly the same number of samples in the given full_dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "885a7caf64358c8d4c40e9211c061847",
     "grade": false,
     "grade_id": "cell-task-1-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Make sure you write your solution to this task below in this cell only.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import pandas as pd\n",
    "X = full_dataset.iloc[:,:-1]\n",
    "y = full_dataset.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "578f4cda174dfd0be1caa2bfbc296293",
     "grade": true,
     "grade_id": "cell-task-1-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Let me test your code above\n",
    "assert X.shape==(101400,36)\n",
    "assert y.shape==(101400,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4.3750\n",
       "1    6.9375\n",
       "2    8.5000\n",
       "3    8.5000\n",
       "4    9.0000\n",
       "Name: BWEIGHT, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "392aa1e5c4af6d6bba52a29c5f4f4532",
     "grade": false,
     "grade_id": "cell-14a6200ac36dbf52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2:\n",
    "* Given X representing the input matrix from the full_dataset, y being the target vector (the rightmost column of the full_dataset), obtained from Task 1: \n",
    "* randomly split the (X,y) dataset into 75% for training and 25% for testing using the library function from the library [sklearn.model_selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) . Please pass to the train_test_split function an additional argument random_state=45931.\n",
    "* Store the 4 splits as X_train, X_test, y_train, y_test respectively.\n",
    "* Save the ID column for X_train and X_test into ID_train and ID_test as list variable.\n",
    "* Now, drop the ID columns from both X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6975d3d25f767e1375f7e0066bdedca3",
     "grade": false,
     "grade_id": "cell-task-2-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Make sure you write your solution to this task below in this cell only.\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,random_state = 45931)\n",
    "ID_train = X_train[['ID']]\n",
    "ID_test = X_test[['ID']]\n",
    "X_train = X_train.drop(['ID'], axis=1)\n",
    "X_test = X_test.drop(['ID'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76083288a16ecceea724405cad650a4d",
     "grade": true,
     "grade_id": "cell-task-2-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Let me test your code above\n",
    "assert (X_train.shape, X_test.shape, y_train.shape, y_test.shape) == ((76050, 35), (25350, 35), (76050,), (25350,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3:\n",
    "Compute mean, stdev, min, max, 25% percentile, median and 75% percentile of BWEIGHT target variable (i.e, the target y) in the training set (i.e., y_train), and print the computed values as a numpy array containing these 7 results (respectively).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54be1b6065eac85e18d2e6da8c50f758",
     "grade": false,
     "grade_id": "cell-task-3-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.25699786  1.33004969  0.3125     13.0625      6.625       7.375\n",
      "  8.0625    ]\n"
     ]
    }
   ],
   "source": [
    "#Make sure you write your solution to this task below in this cell only.\n",
    "mean_val = 0\n",
    "stdev_val = 0\n",
    "min_val = 0\n",
    "max_val = 0\n",
    "percentile25_val = 0\n",
    "median_val = 0\n",
    "percentile75_val = 0\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "mean_val = np.mean(y_train)\n",
    "stdev_val = np.std(y_train)\n",
    "min_val = np.min(y_train)\n",
    "max_val = np.max(y_train) \n",
    "percentile25_val = y_train.quantile(0.25) \n",
    "median_val = y_train.quantile(0.5) \n",
    "percentile75_val = y_train.quantile(0.75)\n",
    "\n",
    "print( np.array([mean_val,stdev_val, min_val,max_val,percentile25_val,median_val,percentile75_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c7a1acc269078d73ca78233815c9dce",
     "grade": true,
     "grade_id": "cell-task-3-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(sum([mean_val,stdev_val, min_val,max_val, percentile25_val,median_val,percentile75_val])-44.024547552217015) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: \n",
    "\n",
    "A little background first: Categorical features are features that contain values that are not numeric. As you can imagine non-numeric values will create trouble (by introducing `nan`) during calculation to gradients, and whatnot, right? The maths are undefined when these get in its way. An obvious solution you may be intrigued to do is dropping the features! Aha! Wrong!! Every piece of data is precious... as those non-numeric features may present with valuable insights of the data samples to find the patterns to map inputs with output/targets. So, we should include them. But, how?\n",
    "\n",
    "The answer is via \"Encoding\" we can make good use the non-numeric variables.\n",
    "\n",
    "There are several types of encoding used in practice. Here are the two popular ones:\n",
    "\n",
    "1. **Label Encoding**, where labels are encoded as subsequent numbers. Say, for a categorical feature named \"Category\" with three categorical values: {“Cat”, “Dog” or “Zebra”} can be encoded to \"0\", \"1\", \"2\" respectively as in figure below. The issue with this type of encoding may unintentionally impose a type of ordering of the categories, that may add bias to the training.\n",
    "![label-encoding](http://54.160.44.72/STATIC_FIGS_DO_NOT_MOVE/le.png)\n",
    "2. **One Hot Encoding**, ignores the ordering of the categories all together. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1. Also, don't forget to remove the original categorical features. Here below just an example, how to convert the categorical feature called \"Category\" having the {“Cat”, “Dog” or “Zebra”} values into three new binary features: \"Cat\", \"Dog\", \"Zebra\".\n",
    "![label-encoding](http://54.160.44.72/STATIC_FIGS_DO_NOT_MOVE/ohe.png)\n",
    "\n",
    "**A note on the Dummy Variable Trap**\n",
    "The Dummy Variable Trap occurs when two or more dummy variables created by one-hot encoding are highly correlated (i.e., becomes multi-collinear). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models. In other words, the individual effect of the dummy variables on the prediction model can not be interpreted well because of multicollinearity.\n",
    "\n",
    "Using the one-hot encoding method, a new dummy variable is created for each categorical variable to represent the presence (1) or absence (0) of the categorical variable. For example, if tree species is a categorical variable made up of the values pine, or oak, then tree species can be represented as a dummy variable by converting each variable to a one-hot vector. This means that a separate column is obtained for each category, where the first column represents if the tree is pine and the second column represents if the tree is oak. Each column will contain a 0 or 1 if the tree in question is of the column's species. These two columns are multi-collinear since if a tree is pine, then we know it's not oak and vice versa. The machine learning models trained on dataset having this multi-collinearity suffers. A remedy is to drop first (or any one) of the dummy (i.e., one-hot) features created.\n",
    "\n",
    "Given the training dataset (X_train, y_train), save as X_train_ohe after replacing all the non-numeric variables (i.e., categorical variables) with numeric encoding. Also, you need to use the same encoding to work on the X_train dataset as well and save it as X_test_oh. We need to make sure any new categorical value in the test dataset get ignored, i.e, when an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Please complete the following `almost complete` function `lets_do_one_hot_encoding()` that encodes dataset on the supplied categorical feature column list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1bc273ec89d4b9744918febfb904ca5",
     "grade": false,
     "grade_id": "cell-task-4-ans-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Make sure you write your solution to this task below in the marked location in this cell only.\n",
    "\n",
    "\n",
    "def lets_do_one_hot_encoding(data, categorical_features, transform_only=True, encoders=[], verbose=False):\n",
    "    '''\n",
    "    The function does one_hot_encoding on the given dataset... \n",
    "    It's intentionally defined fully, meaning you do not need to do anything here... but show us\n",
    "    how to use it properly.\n",
    "    \n",
    "    Input: \n",
    "        * data -- it's pretty much either X_train, or X_test that you prepared previously, that is\n",
    "                  any dataframe having independent variables.\n",
    "        * categorical_features -- a list of column/feature names in the data (dataframe) that you think\n",
    "                  are non-numerical / i.e., categorical that you want to be converted to numerical\n",
    "                  using the one-hot encoding technique.\n",
    "        * transform_only -- a boolean parameter, if set to False, will learn (i.e., fit) various categorical\n",
    "                values in the categorical_features from the given dataset, and use this to encode the dataset\n",
    "                using one-hot encoding. This is important that you set to False on training dataset, and\n",
    "                True on test set. If new categorical values are present in the test dataset, those will be\n",
    "                ignored, making it easier to have same set of encoded features both in training and test \n",
    "                dataset... otherwise, subsequent operations (may/) will not work.\n",
    "        * encoders -- a list of one-hot encoders previously saved, and now will be used to encode given dataset.\n",
    "                If transform_only=True, the function looks for this provided list of encoders to encode the \n",
    "                dataset instead of learning new categories. Once again, it's expected that you pass the set\n",
    "                of encoders for each of the categorical features that you encoded the training set -- i.e.,\n",
    "                leave it empty for training set, and pass the set of encoders while encoding test set.\n",
    "    Returns:\n",
    "        * data -- the converted dataframe after the encoding is completed.\n",
    "        * enc_list -- is the list of encoders used to encode the given data.\n",
    "        * categorical_features -- is the list of categorical features that you would want to encode.\n",
    "        It's expected that for training dataset, you save the encoder list and the list of features for later\n",
    "        use to encode a test dataset.\n",
    "    '''\n",
    "    if len(encoders)>0:\n",
    "        enc_list=encoders\n",
    "    else:\n",
    "        enc_list = []\n",
    "    col_onehot = []\n",
    "    i = 0\n",
    "    for feature in categorical_features:\n",
    "        if verbose: print('Dealing with feature ={}'.format(feature))\n",
    "        if transform_only==True: #for test dataset\n",
    "            enc = enc_list[i]\n",
    "            c_onehot = enc.transform(data[[feature]])\n",
    "            i = i + 1\n",
    "        else: #fit and transform\n",
    "            enc = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n",
    "            c_onehot = enc.fit_transform(data[[feature]])\n",
    "            if verbose: print('c_onehot = {}'.format(c_onehot))\n",
    "            enc_list.append(enc)\n",
    "        c_onehot = pd.DataFrame(c_onehot, columns=list(enc.categories_[0][1:])) #dropped first column\n",
    "        if verbose: print('dropped first column...c_onehot = {}'.format(c_onehot))\n",
    "        c_onehot = c_onehot.add_prefix(feature+'_')\n",
    "        col_onehot.append(c_onehot)\n",
    "        if verbose: print('col_onehot = {}'.format(col_onehot))\n",
    "    \n",
    "    #concat all onehot feature columns\n",
    "    concat_df = pd.concat(col_onehot,axis=1)\n",
    "    #match index with given data\n",
    "    concat_df.index = data.index\n",
    "    \n",
    "    #Here below is your task:\n",
    "    #i) drop the categorical feature columns from dataframe `data`\n",
    "    #ii) and then merge with those new onehot features stored in `cocat_df`\n",
    "    # YOUR CODE HERE\n",
    "    for feature in categorical_features:\n",
    "        data =data.drop(feature, axis='columns')\n",
    "    data =pd.concat([data, concat_df], axis=1, join=\"inner\")\n",
    "   \n",
    "    \n",
    "    return data,enc_list,categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcd60b8e824b91dd079ddc3847f66d4a",
     "grade": false,
     "grade_id": "cell-task-4-ans-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "let's apply one-hot encoding on two columns: \"HISPMOM\" and \"HISPDAD\" of the training dataset, X_train.\n",
    "Please call the function, lets_do_one_hot_encoding() you helped defining above passing appropriate arguments.\n",
    "Also, receive the 3 return values from the function call: X_train_ohe, enc_list and categorical_features to \n",
    "use them in the next question, that is, encoding the test dataset, X_test, and \n",
    "later in Task 17 to encode the judge set. \n",
    "\n",
    "It's always a good idea to save the list of encoders and features in a file (e.g., joblib), if you would \n",
    "want to apply your model to evaluate/predict on a new test sample. In a summary: if your model was trained with\n",
    "transformed one-hot encoded dataset, a new test data also needs to be encoded with the same list of variables\n",
    "defined by the encoding.\n",
    "\n",
    "'''\n",
    "X_train_ohe = []  #one-hot encoded training dataset\n",
    "enc_list = [] #encoder list to be created during one-hot encoding of the training dataset\n",
    "categorical_features = ['HISPMOM', 'HISPDAD'] #The list of categorical features in question.\n",
    "\n",
    "#Here below is your task:one-hot encoding of train set: X_train\n",
    "# YOUR CODE HERE\n",
    "X_train_ohe, enc_list, categorical_features= lets_do_one_hot_encoding(X_train, categorical_features, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5a33654dfce743876a78b7af8234a2c",
     "grade": false,
     "grade_id": "cell-task-4-ans-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "let's apply one-hot encoding on the same two columns: \"HISPMOM\" and \"HISPDAD\" of the test dataset, X_test.\n",
    "Please call the function, lets_do_one_hot_encoding() you helped defining above with appropriate parameters.\n",
    "Be sure to use the encoders from the training set and turn on the `transform_only` switch to true.\n",
    "Also, receive the 3 return values from the function call in X_test_ohe, enc_list, categorical_features. \n",
    "Although, the second and third returned values can be ignored.\n",
    "\n",
    "Please make sure, the one-hot encoded training set and test set has the exact same number of features/columns,\n",
    "and in the same order. If not, the test in the next cell will almost certainly fail, and you will lose points,\n",
    "and the training and testing of the machine learning model will absolutely fail. So, please take close\n",
    "attention.\n",
    "\n",
    "'''\n",
    "X_test_ohe = []  #one-hot encoded test dataset\n",
    "\n",
    "#Here below is your task: one-hot encoding of test set: X_test\n",
    "# YOUR CODE HERE\n",
    "\n",
    "X_test_ohe, enc_list, categorical_features= lets_do_one_hot_encoding(X_test, categorical_features, True, enc_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6de7725f134cdf17af5d707828fc4938",
     "grade": true,
     "grade_id": "cell-task-4",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(X_train_ohe.columns)== len(X_test_ohe.columns) and len(X_train_ohe.columns) == sum([1 for i, j in zip(X_train_ohe.columns, X_test_ohe.columns) if i == j])\n",
    "assert (X_train_ohe.shape, X_test_ohe.shape)==((76050, 45), (25350, 45))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "380f16a447c5bbf360bfc98ade3ec334",
     "grade": false,
     "grade_id": "cell-4f6691287ee0e8d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 5: \n",
    "* Given the X_train_ohe (Onehot encoded Pandas Dataframe from Task 4), check if there are missing values, and if yes, count how many, and impute the missing values with corresponding mean values. \n",
    "* Finally, print the counting result as a Pandas dataframe named \"missing_counts\" having 2 columns {variable_name,num_of_missing_values).  Please make sure that the result lists all the input variables in the given dataset. \n",
    "* Now, impute the missing values by mean of the respective variable and save the revised dataframe as X_train_ohe_imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b498c3a8b04c31d9250b76413c2f35cc",
     "grade": false,
     "grade_id": "cell-task-5-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/fqk30xsj2tg4p5hwsm8v1qsc0000gn/T/ipykernel_71457/1363415992.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_ohe_imputed[i][j]= mean\n",
      "/var/folders/8w/fqk30xsj2tg4p5hwsm8v1qsc0000gn/T/ipykernel_71457/1363415992.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_ohe_imputed[i][j]= mean\n",
      "/var/folders/8w/fqk30xsj2tg4p5hwsm8v1qsc0000gn/T/ipykernel_71457/1363415992.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_ohe_imputed[i][j]= mean\n",
      "/var/folders/8w/fqk30xsj2tg4p5hwsm8v1qsc0000gn/T/ipykernel_71457/1363415992.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_ohe_imputed[i][j]= mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  variable_name  num_of_missing_values\n",
      "0        GAINED                      1\n",
      "1         FEDUC                      1\n",
      "2         WEEKS                      1\n",
      "3        CIGNUM                      1\n",
      "4        HYDRAM                      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/fqk30xsj2tg4p5hwsm8v1qsc0000gn/T/ipykernel_71457/1363415992.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_ohe_imputed[i][j]= mean\n"
     ]
    }
   ],
   "source": [
    "missing_counts = pd.DataFrame()\n",
    "X_train_ohe_imputed = X_train_ohe.copy()\n",
    "\n",
    "\n",
    "#Your task below. \n",
    "# i) Please prepare the missing_counts dataframe accordingly.\n",
    "# ii) impute missing value in a variable with corresponding mean of the variable.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "lis =[]\n",
    "for i in X_train_ohe_imputed.columns:\n",
    "    if X_train_ohe_imputed[[i]].isnull().values.any():\n",
    "        mean= X_train_ohe_imputed[i].mean()\n",
    "        x =[]\n",
    "        x.append(i)\n",
    "        ct =0\n",
    "        for j in X_train_ohe_imputed.index:\n",
    "            if np.isnan(X_train_ohe_imputed[i][j]):\n",
    "                X_train_ohe_imputed[i][j]= mean\n",
    "                ct = ct+1\n",
    "        x.append(ct)\n",
    "        lis.append(x)\n",
    "    \n",
    "missing_counts = pd.DataFrame(data = lis, columns = ['variable_name', 'num_of_missing_values'])\n",
    "print(missing_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f72d459d000b661a7f2e47b974252d3",
     "grade": true,
     "grade_id": "cell-task-5-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert sum(missing_counts['num_of_missing_values'])==5\n",
    "assert np.isnan(X_train_ohe.loc[[1748]][['FEDUC']].iloc[0,0])==True\n",
    "assert np.isnan(X_train_ohe_imputed.loc[[1748]][['FEDUC']].iloc[0,0])==False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea876b435a5e6a04eacd38ca9b46a9c0",
     "grade": false,
     "grade_id": "cell-f1b2e2f3d9c5867f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 6: \n",
    "* Given a X_train_ohe_imputed (Pandas dataframe from Task 5) where all the categorical variables are already replaced with numeric values, print a list of top 20 highly correlated variables with respect to the target variable, and save the result as a Pandas dataframe named top20_df with 2 columns {variable,corr_score}. \n",
    "* Here, the corr_score between a variable x and the target variable y needs to be computed using the Pearson Correlation Coefficient (PCC). Please note, PCC ranges between -1 to +1. PCC score 0 means no correlation, while value towards +1 and -1 represent positive and negative correlations respectively. For instance, PCC=0.8 and PCC=-0.8 tell similar strength positive and negative correlations between the two subject variables.\n",
    "* Please do not include BWEIGHT in the top20_df list of top 20 correlated variable list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d2be22a8edc85bcac7419ab9cae7942",
     "grade": false,
     "grade_id": "cell-task-6-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>corr_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WEEKS</td>\n",
       "      <td>0.565254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAINED</td>\n",
       "      <td>0.174466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VISITS</td>\n",
       "      <td>0.129247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HYPERPR</td>\n",
       "      <td>0.111375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MARITAL</td>\n",
       "      <td>0.106297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SEX</td>\n",
       "      <td>0.091260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CIGNUM</td>\n",
       "      <td>0.086711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RACEDAD</td>\n",
       "      <td>0.084284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RACEMOM</td>\n",
       "      <td>0.078811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PRETERM</td>\n",
       "      <td>0.075237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CERVIX</td>\n",
       "      <td>0.068751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MAGE</td>\n",
       "      <td>0.068550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PINFANT</td>\n",
       "      <td>0.065963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ECLAMP</td>\n",
       "      <td>0.065429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MEDUC</td>\n",
       "      <td>0.055249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FAGE</td>\n",
       "      <td>0.051981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FEDUC</td>\n",
       "      <td>0.051537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HYDRAM</td>\n",
       "      <td>0.050019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HYPERCH</td>\n",
       "      <td>0.047094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UTERINE</td>\n",
       "      <td>0.040339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable  corr_score\n",
       "0     WEEKS    0.565254\n",
       "1    GAINED    0.174466\n",
       "2    VISITS    0.129247\n",
       "3   HYPERPR    0.111375\n",
       "4   MARITAL    0.106297\n",
       "5       SEX    0.091260\n",
       "6    CIGNUM    0.086711\n",
       "7   RACEDAD    0.084284\n",
       "8   RACEMOM    0.078811\n",
       "9   PRETERM    0.075237\n",
       "10   CERVIX    0.068751\n",
       "11     MAGE    0.068550\n",
       "12  PINFANT    0.065963\n",
       "13   ECLAMP    0.065429\n",
       "14    MEDUC    0.055249\n",
       "15     FAGE    0.051981\n",
       "16    FEDUC    0.051537\n",
       "17   HYDRAM    0.050019\n",
       "18  HYPERCH    0.047094\n",
       "19  UTERINE    0.040339"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_df = pd.DataFrame() #You need to save the top-20 most correlated variables with respect to BWEIGHT target\n",
    "\n",
    "# Your task below is:\n",
    "# i) on the imputed dataset you got from task 5, find the top-20 most-correlated variables\n",
    "# with respect to the target variable.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "Xy_train = X_train_ohe_imputed.join(y_train)\n",
    "corr_matrix = Xy_train.corr(method='pearson')\n",
    "# Correlation with output variable\n",
    "corr_target = abs(corr_matrix[[\"BWEIGHT\"]])\n",
    "corr_target_sorted = corr_target.sort_values(by=\"BWEIGHT\", ascending=False)\n",
    "top20_variables = corr_target_sorted.index[1:21] \n",
    "top20_df = pd.DataFrame({'variable':top20_variables,'corr_score':list(corr_target_sorted.iloc[1:21,0])})\n",
    "top20_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56326e76b1d8040d5396df8a0430dda8",
     "grade": true,
     "grade_id": "cell-task-6-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert top20_df.shape==(20,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb50eb6174104b29ec5dbad15a0cbfc1",
     "grade": false,
     "grade_id": "cell-13bacca8345d288f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 7: \n",
    "Given the X_train_ohe_imputed (as Pandas dataframe from task 5) and and top20_df (as Pandas Dataframe from Task 6) having 2 columns {variable_name,corr_score} similar to the one you computed in Task 6:\n",
    "* Please save as X_train_t20 keeping only the columns listed in the top20_df dataframe.\n",
    "* Repeat the process for X_test_ohe (obtained from task 4), and save it as X_test_t20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88f870a150d22a98cc8f02ae4b35d9bf",
     "grade": false,
     "grade_id": "cell-task-7-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your task below is to slice training and test dataset to retain only the top-20 most-correlated variables\n",
    "# with respect to the target variable\n",
    "\n",
    "X_train_t20 = pd.DataFrame()\n",
    "X_test_t20 = pd.DataFrame()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_train_t20 = X_train_ohe_imputed[top20_df.iloc[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WEEKS', 'GAINED', 'VISITS', 'HYPERPR', 'MARITAL', 'SEX', 'CIGNUM',\n",
       "       'RACEDAD', 'RACEMOM', 'PRETERM', 'CERVIX', 'MAGE', 'PINFANT', 'ECLAMP',\n",
       "       'MEDUC', 'FAGE', 'FEDUC', 'HYDRAM', 'HYPERCH', 'UTERINE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_t20 = X_test_ohe[top20_df.iloc[:,0]]\n",
    "X_test_t20.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3b2fed52259e32d65adbb836605695b",
     "grade": true,
     "grade_id": "cell-task-7-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert (X_train_t20.shape,X_test_t20.shape)==((76050, 20), (25350, 20))\n",
    "assert len(X_train_t20.columns)== len(X_test_t20.columns) and len(X_train_t20.columns) == sum([1 for i, j in zip(X_train_t20.columns, X_test_t20.columns) if i == j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39ff6907d36c22a1ab31e2f3e5ca41e3",
     "grade": false,
     "grade_id": "cell-c10878379c304ed1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 8: \n",
    "* Apply min-max scaling on the training dataset (X_train_t20 obtained from Task 7). Save the result as X_train_scaled_mm.\n",
    "* Then scale the test dataset (X_test_t20 obtained from Task 7) based on the metrics you obtain when you scale the training dataset. Save the result as X_test_scaled_mm.\n",
    "* PLEASE DO NOT SCALE y_train and y_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3330f656f4a96262a427f25e82dc201",
     "grade": false,
     "grade_id": "cell-task-8-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled_mm = np.array(X_train_t20)\n",
    "X_test_scaled_mm = np.array(X_test_t20)\n",
    "\n",
    "#Your task below:\n",
    "# YOUR CODE HERE\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled_mm = scaler.fit_transform(X_train_t20) \n",
    "X_test_scaled_mm = scaler.transform(X_test_t20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3b897911d136382956cab90457961e4",
     "grade": true,
     "grade_id": "cell-task-8-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(sum(np.array([min(X_train_scaled_mm[:,0]),max(X_train_scaled_mm[:,0]),min(X_test_scaled_mm[:,0]),max(X_test_scaled_mm[:,0])])-np.array([0.0,1.0,0.0,1.0])))<1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd6dd3a6052de005b3e19471e2791aca",
     "grade": false,
     "grade_id": "cell-cdf4302e8af02822",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 9: \n",
    "* Apply standardization (i.e., normalization) scaling on the training dataset (X_train_t20 obtained from Task 7). Save the result as X_train_scaled_std.\n",
    "* Then scale the test dataset (X_test_t20 obtained from Task 7) based on the metrics you obtain when you scale the training dataset. Save the result as X_test_scaled_std.\n",
    "* PLEASE DO NOT SCALE y_train and y_test.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "feb85999afd4e1882f06d0421b4d6102",
     "grade": false,
     "grade_id": "cell-task-9-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled_std = np.array(X_train_t20)\n",
    "X_test_scaled_std = np.array(X_test_t20)\n",
    "\n",
    "#Your task below:\n",
    "# YOUR CODE HERE\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_std = scaler.fit_transform(X_train_t20) \n",
    "X_test_scaled_std = scaler.transform(X_test_t20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "613ac9333e8044304d71ad640f25f9e1",
     "grade": true,
     "grade_id": "cell-task-9-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for i in np.arange(20):\n",
    "    assert abs(X_train_scaled_std[:,i].mean()-0.0)<1e-4 and abs(X_train_scaled_std[:,i].std()-1.0)<1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 10: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_closed_form_training** : It fits a linear regression model using the closed-form solution to obtain the coefficients, beta's, as a numpy array of m+1 values (Please recall class lecture), where *m* is the number of variables kept in X_train (the first argument to the function). Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_closed_form_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function.\n",
    "* **RMSLE**: It takes two lists: y_test, y_pred, where the first list represents ground truth (i.e., actual) target values for the given samples, and the second list represents a corresponding predicted values for exactly same number of samples in y_test. Compute and return the Root Mean Squared Logarithmic Error (RMSLE) of the prediction. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION or RMSLE calculation.\n",
    "* Now, call linear_regression_closed_form_training() function providing X_train_scaled_std, y_train obtained from Task 9, and save the returned results as betas_closed_form,cpu_time_closed_form.\n",
    "* Print betas_closed_form, cpu_time_closed_form\n",
    "* Call linear_regression_closed_form_predict() function providing betas_closed_form,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSLE() function providing y_test and y_pred. Save returned result as rmsle_closed_form.\n",
    "* Print rmsle_closed_form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd6391855c968c7333a5e6ba89a5b885",
     "grade": false,
     "grade_id": "cell-task-10-ans-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_closed_form_training(X_train, y_train):\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "\n",
    "    #Your work here\n",
    "    # YOUR CODE HERE\n",
    "    X = np.hstack((np.ones(len(X_train)).reshape(len(X_train),1),X_train))\n",
    "    y = y_train\n",
    "    t1_start = perf_counter()\n",
    "    betas = np.dot(np.dot(inv(np.dot(X.T,X)),X.T),y) \n",
    "    t1_stop = perf_counter()\n",
    "    cpu_time = t1_stop - t1_start\n",
    "    \n",
    "    return betas,cpu_time\n",
    "\n",
    "def linear_regression_closed_form_predict(betas, X_test):\n",
    "    y_pred = []\n",
    "    #your work below\n",
    "    # YOUR CODE HERE\n",
    "    X_test_canonical = np.hstack((np.ones(len(X_test)).reshape(len(X_test),1),X_test))\n",
    "    betas_reshaped = betas.reshape(X_test_canonical.shape[1],1)\n",
    "    y_pred = np.dot(X_test_canonical, betas_reshaped)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def RMSLE(y_test, y_pred, verbose=False):\n",
    "    rmsle_val = 0\n",
    "    #Your work below\n",
    "    # YOUR CODE HERE\n",
    "    #y_test = np.array(y_test).reshape(y_pred.shape)\n",
    "    rmsle_val = np.sqrt(np.mean(np.square(np.log1p(y_test)-np.log1p(y_pred))))\n",
    "    return rmsle_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5efb20a92f2cd99f034f47356f513d2e",
     "grade": false,
     "grade_id": "cell-task-10-ans-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16961    9.0000\n",
      "28131    9.0625\n",
      "35114    7.1250\n",
      "85941    6.8125\n",
      "26819    7.1875\n",
      "          ...  \n",
      "62226    8.6250\n",
      "87904    8.5625\n",
      "32338    8.4375\n",
      "27127    8.0625\n",
      "28789    8.4375\n",
      "Name: BWEIGHT, Length: 76050, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 7.25699786,  0.71519322,  0.16451626,  0.03403515, -0.08264272,\n",
       "        -0.07634788, -0.12961155, -0.09253333, -0.05847441, -0.04475425,\n",
       "        -0.03880411, -0.02739149,  0.10283456,  0.0723325 , -0.03912292,\n",
       "        -0.01662473,  0.00921883, -0.01519333, -0.04398384, -0.02638443,\n",
       "        -0.01920356]),\n",
       " 0.008567458999991118)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_closed_form = []\n",
    "cpu_time_closed_form = 0\n",
    "#Your task here, call appropriate function to get these two\n",
    "# YOUR CODE HERE\n",
    "print(y_train)\n",
    "betas_closed_form, cpu_time_closed_form = linear_regression_closed_form_training(X_train_scaled_std, y_train)\n",
    "betas_closed_form, cpu_time_closed_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ed04c2fb8b801b3b717785676907169",
     "grade": false,
     "grade_id": "cell-task-10-ans-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "#Your task here. call appropriate function to get get y prediction for the supplied test dataset scaled\n",
    "# YOUR CODE HERE\n",
    "y_pred = linear_regression_closed_form_predict(betas_closed_form,X_test_scaled_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a2a1bde964992b0f5d795c868345226",
     "grade": false,
     "grade_id": "cell-task-10-ans-4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rmsle_closed_form = 0\n",
    "#Your task below. Call the function to compute RMSLE score of the y predictions\n",
    "# YOUR CODE HERE\n",
    "#y_pred\n",
    "rmsle_closed_form = RMSLE(y_test, y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04302752be718a467ba8558efcba00dd",
     "grade": true,
     "grade_id": "cell-task-10-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(RMSLE([1,2,3,4,5],[1,1,1,1,1])-0.733674673672524)<1e-4\n",
    "assert abs(RMSLE([100, 200, 300, 400, 500], [90, 190, 290, 390, 490])-0.05596497907273607)<1e-4\n",
    "assert (betas_closed_form.shape[0],y_pred.shape[0])==(21,25350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e18751f816ea425fd0ba1862ac9dd7e0",
     "grade": false,
     "grade_id": "cell-0fc07a9d9c84d1f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 11: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_batch_training** : It fits a linear regression model using the batch gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Make sure you compute average of gradients in the batch. Please use the alpha (i.e, the learning rate) and nEpoch (number of epochs) parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_gd_batch_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_batch_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.01,nEpoch=1000, and save the returned results as betas_batch,cpu_time_batch.\n",
    "* Print betas_batch, cpu_time_batch\n",
    "* Call linear_regression_gd_batch_predict() function providing betas_batch,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSLE() function providing y_test and y_pred. Save returned result as rmsle_batch.\n",
    "* Print rmsle_batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea60ad964c0407c62b0ff7b4c103946a",
     "grade": false,
     "grade_id": "cell-task-11-ans-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_gd_batch_training(X_train,y_train, alpha, nEpoch):\n",
    "    random.seed(554433)\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "    epsilon = 1e-6\n",
    "    #Your task below\n",
    "    # YOUR CODE HERE\n",
    "    y_pred = []\n",
    "    y = np.array(y_train).reshape((len(y_train),1))\n",
    "    X = np.array(X_train)\n",
    "    if all(X[:,0]==1) == False:\n",
    "        X = np.hstack((np.ones(len(X)).reshape(len(X), 1), X))\n",
    "    betas = np.random.rand(X.shape[1]).reshape((X.shape[1],1))\n",
    "    convergence_reached = False\n",
    "    counter = 0\n",
    "    betas_prev = betas\n",
    "    \n",
    "    t1_start = perf_counter()\n",
    "    while not convergence_reached:\n",
    "        #Compute grad\n",
    "        error = np.dot(X, betas) - y\n",
    "        grad = (1/len(X))*np.dot(X.T, error)\n",
    "        betas = betas - alpha * grad\n",
    "        if counter > nEpoch:\n",
    "            convergence_reached = True\n",
    "        elif np.sum(np.abs(betas-betas_prev)) < epsilon:\n",
    "            convergence_reached = True\n",
    "        betas_prev = betas\n",
    "        counter = counter + 1\n",
    "    t1_stop = perf_counter()\n",
    "    cpu_time = t1_stop - t1_start    \n",
    "\n",
    "    return betas, cpu_time\n",
    "\n",
    "def linear_regression_gd_batch_predict(betas,X_test):\n",
    "    y_pred = []\n",
    "    \n",
    "    #Your task below\n",
    "    # YOUR CODE HERE\n",
    "    X_test_canonical = np.hstack((np.ones(len(X_test)).reshape(len(X_test),1),X_test))\n",
    "    betas_reshaped = betas.reshape(X_test_canonical.shape[1],1)\n",
    "    y_pred = np.dot(X_test_canonical, betas_reshaped)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6609b191909ab4e3692446219c6271f2",
     "grade": false,
     "grade_id": "cell-task-11-ans-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 7.25670258],\n",
       "        [ 0.71507003],\n",
       "        [ 0.16382044],\n",
       "        [ 0.03411903],\n",
       "        [-0.08254211],\n",
       "        [-0.07633532],\n",
       "        [-0.12969601],\n",
       "        [-0.09306953],\n",
       "        [-0.0622522 ],\n",
       "        [-0.04157257],\n",
       "        [-0.0387495 ],\n",
       "        [-0.02735647],\n",
       "        [ 0.08627115],\n",
       "        [ 0.07252266],\n",
       "        [-0.03919517],\n",
       "        [-0.00929934],\n",
       "        [ 0.02460225],\n",
       "        [-0.02011973],\n",
       "        [-0.04410028],\n",
       "        [-0.02625073],\n",
       "        [-0.01915461]]),\n",
       " 0.6940950420000007)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_batch = []\n",
    "cpu_time_batch = 0\n",
    "\n",
    "#your task below\n",
    "# YOUR CODE HERE\n",
    "betas_batch,cpu_time_batch =linear_regression_gd_batch_training(X_train_scaled_std, y_train,alpha=0.01,nEpoch=1000)\n",
    "betas_batch, cpu_time_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a92e4fabdb14e6f5d7dc1ab6b63b336",
     "grade": false,
     "grade_id": "cell-task-11-ans-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "#your task below\n",
    "# YOUR CODE HERE\n",
    "y_pred = linear_regression_gd_batch_predict(betas_batch,X_test_scaled_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0763f25cbcebfeaa938a1df858f1dc8",
     "grade": false,
     "grade_id": "cell-task-11-ans-4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1390895874686932\n"
     ]
    }
   ],
   "source": [
    "rmsle_batch = 0\n",
    "# YOUR CODE HERE\n",
    "#print(y_test)\n",
    "rmsle_batch = RMSLE(y_test,y_pred.flatten())\n",
    "print(rmsle_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4eec4b5843097a0f78b3607a3542513e",
     "grade": true,
     "grade_id": "cell-task-11-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b286b81665867b4b8ab2f944c24d4fc",
     "grade": false,
     "grade_id": "cell-730ed0725e4d67b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 12:\n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_stochastic_training** : It fits a linear regression model using the stochastic gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate), nEpoch (number of epochs), nIteration (number of iterations) parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_gd_stochastic_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_stochastic_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.001,nEpoch=100, nIteration=10000 , and save the returned results as betas_stochastic,cpu_time_stochastic.\n",
    "* Print betas_stochastic, cpu_time_stochastic\n",
    "* Call linear_regression_gd_stochastic_predict() function providing betas_stochastic,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSLE() function providing y_test and y_pred. Save returned result as rmsle_stochastic.\n",
    "* Print rmsle_stochastic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b1708cfa018faa1be5a7101cf242a9c",
     "grade": false,
     "grade_id": "cell-task-12-ans-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_gd_stochastic_training(X_train,y_train,alpha,nEpoch,nIteration):\n",
    "    random.seed(554433)\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "    # YOUR CODE HERE\n",
    "    y = np.array(y_train).reshape((len(y_train),1))\n",
    "    X = np.array(X_train)\n",
    "    if all(X[:,0]==1) == False:\n",
    "        X = np.hstack((np.ones(len(X)).reshape(len(X), 1), X))\n",
    "    betas = np.random.rand(X.shape[1]).reshape((X.shape[1],1))\n",
    "    convergence_reached = False\n",
    "    betas_prev = betas\n",
    "    t1_start = perf_counter()\n",
    "    counter = 0\n",
    "    for epoch in np.arange(nEpoch):\n",
    "        np.random.shuffle(X)\n",
    "        for i in np.arange(len(X)):\n",
    "            Xi = X[i,:].reshape((1,len(X[i,:])))\n",
    "            error = np.dot(Xi,betas)-y[i]\n",
    "            grad = np.dot(Xi.T,error)\n",
    "            betas = betas - alpha * grad\n",
    "            if counter > nIteration:\n",
    "                convergence_reached = True\n",
    "            elif np.sum(np.abs(betas-betas_prev)) < epsilon:\n",
    "                convergence_reached = True\n",
    "            betas_prev = betas\n",
    "            counter = counter + 1\n",
    "            if convergence_reached:\n",
    "                break\n",
    "        if convergence_reached:\n",
    "            break\n",
    "    t1_stop = perf_counter()\n",
    "    cpu_time = t1_stop - t1_start\n",
    "    return betas, cpu_time\n",
    "\n",
    "def linear_regression_gd_stochastic_predict(betas, X_test):\n",
    "    y_pred = []\n",
    "    \n",
    "    #your code below\n",
    "    # YOUR CODE HERE\n",
    "    X_test_canonical = np.hstack((np.ones(len(X_test)).reshape(len(X_test),1),X_test))\n",
    "    beta_reshaped = betas.reshape(X_test_canonical.shape[1],1)\n",
    "    y_pred = np.dot(X_test_canonical, beta_reshaped)\n",
    "    return y_pred\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "778fac12e7b54354c1246162815e78ac",
     "grade": false,
     "grade_id": "cell-task-12-ans-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.97961092]\n",
      " [-0.57493023]\n",
      " [-0.16101633]\n",
      " [ 0.34625255]\n",
      " [ 0.24964117]\n",
      " [ 0.45064029]\n",
      " [ 0.50582173]\n",
      " [ 0.09562893]\n",
      " [ 0.04002016]\n",
      " [-0.21979051]\n",
      " [ 0.56013062]\n",
      " [ 0.43619922]\n",
      " [ 0.02672886]\n",
      " [-0.59437826]\n",
      " [ 0.53134123]\n",
      " [-0.13312998]\n",
      " [-0.61233011]\n",
      " [-0.17782757]\n",
      " [ 0.15519426]\n",
      " [ 0.54494622]\n",
      " [ 1.99668838]] 0.08049354099999562\n"
     ]
    }
   ],
   "source": [
    "betas_stochastic = []\n",
    "cpu_time_stochastic = 0\n",
    "#your code below\n",
    "# YOUR CODE HERE\n",
    "betas_stochastic, cpu_time_stochastic =linear_regression_gd_stochastic_training(X_train_scaled_std,y_train,\n",
    "alpha=0.01,nEpoch=50,nIteration=100)\n",
    "print(betas_stochastic,cpu_time_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50d8bc2a1a464179d7ed85dbe4adf430",
     "grade": false,
     "grade_id": "cell-task-12-ans-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "#your code below\n",
    "# YOUR CODE HERE\n",
    "y_pred =linear_regression_gd_stochastic_predict(betas_stochastic,X_test_scaled_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08433c9513f95a2532c44d52ac10c734",
     "grade": false,
     "grade_id": "cell-task-12-ans-4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5352490494089946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/fqk30xsj2tg4p5hwsm8v1qsc0000gn/T/ipykernel_71457/3094196268.py:31: RuntimeWarning: invalid value encountered in log1p\n",
      "  rmsle_val = np.sqrt(np.mean(np.square(np.log1p(y_test)-np.log1p(y_pred))))\n"
     ]
    }
   ],
   "source": [
    "rmsle_stochastic = 0\n",
    "#your code below\n",
    "# YOUR CODE HERE\n",
    "rmsle_stochastic = RMSLE(y_test,y_pred.flatten())\n",
    "print(rmsle_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0fda290e2c7fd3344b626219e40b8f4",
     "grade": true,
     "grade_id": "cell-task-12-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0687acc396050f1b200cabf06db9f8ec",
     "grade": false,
     "grade_id": "cell-4a7a0dcac2a0787a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 13: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_minibatch_training** : It fits a linear regression model using the minibatch gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate), nEpoch (number of epochs), nIteration (number of iterations), and batch_size parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_gd_minibatch_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_minibatch_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.01,nEpoch=50, nIteration=1000,batch_size=32, and save the returned results as betas_minibatch,cpu_time_minibatch.\n",
    "* Print betas_minibatch, cpu_time_minibatch\n",
    "* Call linear_regression_gd_minibatch_predict() function providing betas_minibatch,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSLE() function providing y_test and y_pred. Save returned result as rmsle_minibatch.\n",
    "* Print rmsle_minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eddd43fc0fa6257069853b8c1d0052db",
     "grade": false,
     "grade_id": "cell-task-13-ans-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_gd_minibatch_training(X_train,y_train,alpha,nEpoch, nIteration, batch_size):\n",
    "    random.seed(554433)\n",
    "    cpu_time = 0\n",
    "    epsilon = 1e-6\n",
    "    betas = []\n",
    "    ## your code below\n",
    "    # YOUR CODE HERE\n",
    "    y = np.array(y_train).reshape((len(y_train),1))\n",
    "    X = np.array(X_train)\n",
    "    if all(X[:,0]==1) == False:\n",
    "        X = np.hstack((np.ones(len(X)).reshape(len(X), 1), X))\n",
    "    betas = np.random.rand(X.shape[1]).reshape((X.shape[1],1))\n",
    "    convergence_reached = False\n",
    "    betas_prev = betas\n",
    "    t1_start = perf_counter()\n",
    "    counter = 0\n",
    "    for epoch in np.arange(nEpoch):\n",
    "        np.random.shuffle(X)\n",
    "        mini_batches = []\n",
    "        data = np.hstack((X, y))\n",
    "        np.random.shuffle(data)\n",
    "        n_minibatches = data.shape[0] // batch_size\n",
    "        for i in range(n_minibatches + 1):\n",
    "            mini_batch = data[i * batch_size:(i + 1) * batch_size, :]\n",
    "            X_mini = mini_batch[:, :-1]\n",
    "            Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "            mini_batches.append((X_mini, Y_mini))\n",
    "        if data.shape[0] % batch_size != 0:\n",
    "            mini_batch = data[i * batch_size:data.shape[0]]\n",
    "            X_mini = mini_batch[:, :-1]\n",
    "            Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "            mini_batches.append((X_mini, Y_mini))\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini, y_mini = mini_batch\n",
    "            error = np.dot(X_mini, betas) - y_mini\n",
    "            grad = (1 / len(X_mini)) * np.dot(X_mini.T, error)\n",
    "            betas = betas - alpha * grad\n",
    "            if counter > nIteration:\n",
    "                convergence_reached = True\n",
    "            elif np.sum(np.abs(betas-betas_prev)) < epsilon:\n",
    "                convergence_reached = True\n",
    "            betas_prev = betas\n",
    "            counter = counter + 1\n",
    "            if convergence_reached:\n",
    "                break\n",
    "        if convergence_reached:\n",
    "            break\n",
    "    t1_stop = perf_counter()\n",
    "    cpu_time = t1_stop - t1_start\n",
    "    \n",
    "    return betas,cpu_time\n",
    "\n",
    "def linear_regression_gd_minibatch_predict(betas,X_test):\n",
    "    #your solution below\n",
    "    y_pred = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    X_test_canonical = np.hstack((np.ones(len(X_test)).reshape(len(X_test),1),X_test))\n",
    "    betas_reshaped = betas.reshape(X_test_canonical.shape[1],1)\n",
    "    y_pred = np.dot(X_test_canonical, betas_reshaped)\n",
    "    return y_pred\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99753608279111fce360d34cd352899b",
     "grade": false,
     "grade_id": "cell-task-13-ans-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.74002510e+00]\n",
      " [ 1.66308283e-01]\n",
      " [ 1.24785574e-01]\n",
      " [ 1.00500393e-01]\n",
      " [ 2.72755137e-01]\n",
      " [ 5.36125058e-01]\n",
      " [ 1.12396186e-01]\n",
      " [ 6.76591454e-02]\n",
      " [ 3.86744121e-01]\n",
      " [-2.95938780e-01]\n",
      " [ 2.57586420e-01]\n",
      " [ 3.74715698e-03]\n",
      " [ 1.40259163e-01]\n",
      " [ 2.79271008e-01]\n",
      " [ 2.43744419e-01]\n",
      " [ 2.20993131e-01]\n",
      " [ 2.00171151e-01]\n",
      " [ 1.87510042e-02]\n",
      " [ 2.04572476e-01]\n",
      " [ 1.60486625e-02]\n",
      " [ 4.89715410e-02]] 0.17612120799999786\n"
     ]
    }
   ],
   "source": [
    "betas_minibatch = []\n",
    "cpu_time_minibatch = 0\n",
    "#your task below\n",
    "\n",
    "# YOUR CODE HERE\n",
    "betas_minibatch,cpu_time_minibatch =linear_regression_gd_minibatch_training(X_train_scaled_std,y_train,alpha=0.001,nEpoch=50,nIteration=1000,batch_size=32)\n",
    "print(betas_minibatch,cpu_time_minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe4db8429b6307e4954787ff65499ace",
     "grade": false,
     "grade_id": "cell-task-13-ans-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "#your ans below\n",
    "# YOUR CODE HERE\n",
    "y_pred =linear_regression_gd_minibatch_predict(betas_minibatch,X_test_scaled_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "413cca71f418603562fba094f24d2176",
     "grade": false,
     "grade_id": "cell-task-13-ans-4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25350\n",
      "25350\n",
      "0.42784842890056574\n"
     ]
    }
   ],
   "source": [
    "rmsle_minibatch = 0\n",
    "#your ans below\n",
    "# YOUR CODE HERE\n",
    "rmsle_minibatch = RMSLE(y_test,y_pred.flatten())\n",
    "print(len(y_test))\n",
    "print(len(y_pred))\n",
    "print(rmsle_minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45808d918d14da929a25dc50d0dd8783",
     "grade": true,
     "grade_id": "cell-task-13-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64718e3431a40ac46a59475dfaefe21a",
     "grade": false,
     "grade_id": "cell-a1c4ba6f1747a861",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 14:\n",
    "Given the 4 sets of results from the 4 experiments (from Tasks 10, 11, 12, 13) with closed form solution, batch gradient descent, stochastic gradient descent and mini-batch gradient descent, assign a string from the set {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the best predictive performance in terms of RMSE to a variable named `best_name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6044f69e21a43fa33068114a9f49d2",
     "grade": false,
     "grade_id": "cell-task-14-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13907911709676382\n",
      "closed-form\n"
     ]
    }
   ],
   "source": [
    "names = [\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"]\n",
    "rmsle_vals = [rmsle_closed_form, rmsle_batch, rmsle_stochastic, rmsle_minibatch]\n",
    "\n",
    "best_name = 'xxxx'\n",
    "# YOUR CODE HERE\n",
    "print(min(rmsle_vals))\n",
    "least = min(rmsle_vals)\n",
    "for i in range(len(rmsle_vals)):\n",
    "    if rmsle_vals[i] == least:\n",
    "        best_name = names[i]\n",
    "print(best_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f72023bd79fc79bd87d88e9d0f23bb6",
     "grade": true,
     "grade_id": "cell-task-14-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 15: \n",
    "Given the 4 sets of results from the 4 experiments (from Tasks 10, 11, 12, 13) with closed form solution, batch gradient descent, stochastic gradient descent and mini-batch gradient descent, assign a string from the set {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the least training cpu time to a variable called `best_name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bcfaa3a9be1db6493129daea41274d5",
     "grade": false,
     "grade_id": "cell-task-15-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008567458999991118\n",
      "closed-form\n"
     ]
    }
   ],
   "source": [
    "names = [\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"]\n",
    "cpu_times = [cpu_time_closed_form, cpu_time_batch, cpu_time_stochastic, cpu_time_minibatch]\n",
    "best_name = 'xxxx'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(min(cpu_times))\n",
    "least = min(cpu_times)\n",
    "for i in range(len(cpu_times)):\n",
    "    if cpu_times[i] == least:\n",
    "        best_name = names[i]\n",
    "print(best_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55a3c919fdc335ddd8144250d405e0c2",
     "grade": true,
     "grade_id": "cell-task-15-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ae9593dec18690dc8d7e6f0dcde4717",
     "grade": false,
     "grade_id": "cell-45c884bdcfa6ae5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 16: \n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively, \n",
    "* call your implementation of Task 13: minibatch gradient descent based linear regression for each of these learning rates: {0.001, 0.01, 0.05}, batch sizes: {32, 64, 128, 256}\n",
    "    * Please use the nIteration (number of iterations)=100, nEpoch (number of epoch)=50.\n",
    "\n",
    "* For each of the linear regression model, using the computed beta values, predict the test samples provided in the \"X_test_scaled_std\" argument, and let's name your prediction \"y_pred\".\n",
    "* Compute Root Mean Squared Logarithmic Error (RMSLE) of your prediction using the RMSLE() function you defined in Task 10.\n",
    "* Finally, assign the learning rate that shows the best test performance to a variable called `best_alpha`, and also assign as a pandas dataframe named `summary` with 3 columns: {learning_rate, batch_size, test_RMSLE} containing RMSLE's of the 12 linear regression models, i.e., Cartesian product between the alphas and batch_sizes. Also, assign the best performing batch_size to a variable `best_batch_size`.\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d9e8bfac308c18e4b535f6b0aa97a43",
     "grade": false,
     "grade_id": "cell-task-16-ans",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8w/fqk30xsj2tg4p5hwsm8v1qsc0000gn/T/ipykernel_71457/2805722431.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  summary = summary.append(pd.Series([alpha, batch_size, rmsle],index=summary.columns),ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>test_RMSLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.413983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.193673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.195180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.402243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.193472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.050</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.194643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.421847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.193433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.050</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.193365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.418523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.193339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.050</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.193319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  batch_size  test_RMSLE\n",
       "0           0.001        32.0    0.413983\n",
       "1           0.010        32.0    0.193673\n",
       "2           0.050        32.0    0.195180\n",
       "3           0.001        64.0    0.402243\n",
       "4           0.010        64.0    0.193472\n",
       "5           0.050        64.0    0.194643\n",
       "6           0.001       128.0    0.421847\n",
       "7           0.010       128.0    0.193433\n",
       "8           0.050       128.0    0.193365\n",
       "9           0.001       256.0    0.418523\n",
       "10          0.010       256.0    0.193339\n",
       "11          0.050       256.0    0.193319"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = [0.001, 0.01, 0.05]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "summary = pd.DataFrame(columns=['learning_rate', 'batch_size', 'test_RMSLE'])\n",
    "nIteration=1000\n",
    "nEpoch=50\n",
    "\n",
    "best_alpha = -1\n",
    "best_beta = []\n",
    "best_batch_size = -1\n",
    "a = []\n",
    "b = []\n",
    "#your code below\n",
    "# YOUR CODE HERE\n",
    "for batch_size in batch_sizes:\n",
    "    for alpha in alphas:\n",
    "        betas,cpu_time = linear_regression_gd_minibatch_training(X_train_scaled_std,y_train,alpha,nEpoch, nIteration, batch_size)\n",
    "        y_pred =linear_regression_gd_minibatch_predict(betas,X_train_scaled_std)\n",
    "        y_pr = []\n",
    "        for i in range(len(y_pred)):\n",
    "            y_pr.append(y_pred[i][0])\n",
    "            if i == len(y_test) - 1:\n",
    "                break\n",
    "        rmsle = RMSLE(y_test,y_pr)\n",
    "        summary = summary.append(pd.Series([alpha, batch_size, rmsle],index=summary.columns),ignore_index=True)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76f691ee77ca8607951384802dcf6854",
     "grade": true,
     "grade_id": "cell-task-16-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50f666fcbeb7cfe1025b2eb1815cca5f",
     "grade": false,
     "grade_id": "cell-05d685fbd6e9f6a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 17:\n",
    "* Utilizing the best trained linear regression model (so far), predict the target for each of the samples in the judge_dataset.\n",
    "    * I believe you will not forget to do the following before call in the prediction algorithm:\n",
    "        - Save the ID values of the judge dataset into ID_judge and drop it from the judge dataframe.\n",
    "        - Perform onehot encoding using the same encoder you used to encode X_test (Task 4). \n",
    "        - keep only the same top 20 variables as you did in Task 7. \n",
    "        - scale the input variables based on the same metrics you used to scale the training dataset (Task 9). \n",
    "    * Now, call the prediction function of that model to obtain y_pred.\n",
    "    * Prepare a pandas dataframe called `result` having columns: {ID, BWEIGHT}, where ID will the ID of the judge sample, and BWEIGHT is the corresponding y_pred value from your model prediction.\n",
    "    * Save the dataframe as `my_submission.csv`.\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j = judge_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6c54acdff061cda439a2d099fc2e933",
     "grade": false,
     "grade_id": "cell-task-17-ans-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#your task below\n",
    "#i) store the id in a separate variable and drop it from judge dataset\n",
    "#ii) use onehot encoder you obtained to encode the judge set\n",
    "#iii) keep only the top-20 variables in the judge set\n",
    "#iv) scale using the same scalar you obtained before, and save it to judge_scaled_std variable.\n",
    "judge_scaled_std = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ID_judge = X_j[['ID']]\n",
    "X_j = X_j.drop(['ID'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "X_j_ohe = [] \n",
    "\n",
    "X_j_ohe, enc_list, categorical_features= lets_do_one_hot_encoding(X_j, categorical_features, True, enc_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SEX', 'MARITAL', 'FAGE', 'GAINED', 'VISITS', 'MAGE', 'FEDUC', 'MEDUC',\n",
       "       'TOTALP', 'BDEAD', 'TERMS', 'LOUTCOME', 'WEEKS', 'RACEMOM', 'RACEDAD',\n",
       "       'CIGNUM', 'DRINKNUM', 'ANEMIA', 'CARDIAC', 'ACLUNG', 'DIABETES',\n",
       "       'HERPES', 'HYDRAM', 'HEMOGLOB', 'HYPERCH', 'HYPERPR', 'ECLAMP',\n",
       "       'CERVIX', 'PINFANT', 'PRETERM', 'RENAL', 'RHSEN', 'UTERINE',\n",
       "       'HISPMOM_M', 'HISPMOM_N', 'HISPMOM_O', 'HISPMOM_P', 'HISPMOM_S',\n",
       "       'HISPMOM_U', 'HISPDAD_M', 'HISPDAD_N', 'HISPDAD_O', 'HISPDAD_P',\n",
       "       'HISPDAD_S', 'HISPDAD_U'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_j_ohe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_j_t20 = X_j_ohe[top20_df.iloc[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_j_scaled_std = scaler.fit_transform(X_j_t20) \n",
    "len(X_j_scaled_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_j = y_train.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16961    9.0000\n",
       "28131    9.0625\n",
       "35114    7.1250\n",
       "85941    6.8125\n",
       "26819    7.1875\n",
       "          ...  \n",
       "57997    7.5625\n",
       "54695    7.3125\n",
       "43162    8.1875\n",
       "28752    8.0625\n",
       "60913    8.3125\n",
       "Name: BWEIGHT, Length: 2000, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.22837500e+00, -2.44300836e-03, -3.46937583e-02,  1.63984406e-02,\n",
       "        -1.10778140e-02, -2.05528023e-02,  2.84372940e-02,  2.56233973e-02,\n",
       "         2.48665488e-02, -4.27607629e-02, -7.80343360e-04, -3.36039738e-02,\n",
       "        -8.11551877e-02,  4.13151848e-02, -9.69226255e-03, -1.68048278e-02,\n",
       "         1.06386030e-01, -4.87105520e-04,  3.54333621e-02,  3.69630690e-02,\n",
       "         4.14378905e-02]),\n",
       " 0.0009757079999985763)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_closed_form = []\n",
    "cpu_time_closed_form = 0\n",
    "#Your task here, call appropriate function to get these two\n",
    "# YOUR CODE HERE\n",
    "i=0\n",
    "re = []\n",
    "betas_closed_form, cpu_time_closed_form = linear_regression_closed_form_training(X_j_scaled_std, y_j)\n",
    "betas_closed_form, cpu_time_closed_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "#Your task here. call appropriate function to get get y prediction for the supplied test dataset scaled\n",
    "# YOUR CODE HERE\n",
    "y_pred = linear_regression_closed_form_predict(betas_closed_form,X_j_scaled_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.10568381],\n",
       "       [7.18689266],\n",
       "       [7.36203843],\n",
       "       ...,\n",
       "       [7.23738895],\n",
       "       [7.20778647],\n",
       "       [7.24319151]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "faaaf7af257f465290c916f460c84d70",
     "grade": false,
     "grade_id": "cell-task-17-ans-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3809b2709246248e7376ae423002501",
     "grade": false,
     "grade_id": "cell-task-17-ans-3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>BWEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.105684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.186893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.362038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.260398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.141782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   BWEIGHT\n",
       "0   1  7.105684\n",
       "1   2  7.186893\n",
       "2   3  7.362038\n",
       "3   4  7.260398\n",
       "4   5  7.141782"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your task below is to prepare a dataframe `result` with columns: {ID,BWEIGHT} as defined in task-17\n",
    "result = pd.DataFrame()\n",
    "# YOUR CODE HERE\n",
    "result = pd.DataFrame(ID_judge)\n",
    "result['BWEIGHT'] = y_pred\n",
    "result.to_csv('my_submission.csv',index=False)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7f228f5e77d7f23cb28d42edb16c4f7",
     "grade": true,
     "grade_id": "cell-task-17-test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "003a5895debd3edd6684bda330d000d8",
     "grade": false,
     "grade_id": "cell-06ad5ee38a8fe43b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 18: \n",
    "* [Submit my_submission.csv at Kaggle](https://www.kaggle.com/competitions/fall2022-birth-weight-prediction/)\n",
    "* [**Very important**] Please look back from tasks 1-17 to see either do hyperparameter tuning, data analysis, model selection to improve your initial submission.\n",
    "* Please add details about your best submission by assigning appropriate values from your latest Kaggle submission:\n",
    "    * name -- your kaggle userid/handle\n",
    "    * submission_date -- latest date/timestamp of your submission\n",
    "    * score -- the score you got at kaggle for that submission\n",
    "    * entries -- number of entries you made/submitted in Kaggle for this task\n",
    "    * tell_about_the_model -- a string explaining your choice of model, parameters, data analysis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1484be09f8991458af2cf11616e6b78a",
     "grade": true,
     "grade_id": "cell-task-18-ans",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "name = 'Vyanjana Kishaniya'\n",
    "submission_date = '4th October, 2022'\n",
    "score = 0.22294\n",
    "entries = 1\n",
    "tell_about_the_model = 'My choice of model is Closed-form, as it has least CPU_time and least RMSLE values according to task 14 and 15. Also it provides the best solution and finding a closed-form solution is significantly faster than optimizing using an iterative optimization algorithm like gradient descent.'\n",
    "\n",
    "#your task is to re-assign proper values based on the task requirement below.\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "159ce6da369672c22780f9cd7ec3c9ee97460e105c79378255b899da2b6e51d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
